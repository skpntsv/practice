{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "import os\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Input\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import gzip\n",
    "import onnx\n",
    "import tf2onnx\n",
    "import onnxruntime as ort # Для проверки ONNX\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "# --- Класс загрузки данных (ваш класс) ---\n",
    "class MNISTDataLoader:\n",
    "    def __init__(self, data_dir, one_hot=True, class_num=10):\n",
    "        self.data_dir = data_dir\n",
    "        self.one_hot = one_hot\n",
    "        self.class_num = class_num\n",
    "\n",
    "        # Проверяем существование директории\n",
    "        if not os.path.isdir(self.data_dir):\n",
    "            raise FileNotFoundError(f\"Директория данных не найдена: {self.data_dir}\")\n",
    "\n",
    "        print(f\"Загрузка данных из: {self.data_dir}\")\n",
    "        self.train_images, self.train_labels = self._load_data('train')\n",
    "        self.test_images, self.test_labels = self._load_data('t10k')\n",
    "        print(\"Данные загружены.\")\n",
    "        print(f\"  Train: {self.train_images.shape}, {self.train_labels.shape}\")\n",
    "        print(f\"  Test:  {self.test_images.shape}, {self.test_labels.shape}\")\n",
    "\n",
    "        # Добавляем свойство для количества тренировочных примеров\n",
    "        self.num_train_examples = len(self.train_images)\n",
    "\n",
    "    def _load_data(self, prefix):\n",
    "        \"\"\"Load MNIST data from .gz files\"\"\"\n",
    "        image_path = os.path.join(self.data_dir, f'{prefix}-images-idx3-ubyte.gz')\n",
    "        label_path = os.path.join(self.data_dir, f'{prefix}-labels-idx1-ubyte.gz')\n",
    "\n",
    "        if not os.path.exists(image_path) or not os.path.exists(label_path):\n",
    "             raise FileNotFoundError(f\"Файлы данных MNIST не найдены в {self.data_dir} с префиксом '{prefix}'\")\n",
    "\n",
    "        try:\n",
    "            # Load images\n",
    "            with gzip.open(image_path, 'rb') as f:\n",
    "                # Смещение 16 байт для заголовка IDX3\n",
    "                images = np.frombuffer(f.read(), np.uint8, offset=16)\n",
    "                # MNIST изображения 28x28 = 784\n",
    "                images = images.reshape(-1, 784).astype(np.float32) / 255.0\n",
    "                # images = images.reshape(-1, 28, 28, 1).astype(np.float32) / 255.0 # <--- ИЗМЕНЕНО ЗДЕСЬ\n",
    "\n",
    "\n",
    "            # Load labels\n",
    "            with gzip.open(label_path, 'rb') as f:\n",
    "                # Смещение 8 байт для заголовка IDX1\n",
    "                labels = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "\n",
    "            # Convert to one-hot if needed\n",
    "            if self.one_hot:\n",
    "                # Используем tf.keras.utils.to_categorical для надежности\n",
    "                one_hot_labels = tf.keras.utils.to_categorical(labels, num_classes=self.class_num)\n",
    "                # labels = tf.one_hot(labels, self.class_num).numpy() # Альтернатива\n",
    "                return images, one_hot_labels\n",
    "\n",
    "            return images, labels\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка при загрузке или обработке файлов {prefix}: {e}\")\n",
    "            raise\n",
    "\n",
    "# --- Словари классов (как в оригинале) ---\n",
    "dict_2class = {0:'Benign',1:'Malware'}\n",
    "dict_10class_benign = {0:'BitTorrent',1:'Facetime',2:'FTP',3:'Gmail',4:'MySQL',5:'Outlook',6:'Skype',7:'SMB',8:'Weibo',9:'WorldOfWarcraft'}\n",
    "dict_10class_malware = {0:'Cridex',1:'Geodo',2:'Htbot',3:'Miuref',4:'Neris',5:'Nsis-ay',6:'Shifu',7:'Tinba',8:'Virut',9:'Zeus'}\n",
    "dict_20class = {0:'BitTorrent', 1:'Facetime', 2:'FTP', 3:'Gmail', 4:'MySQL',\n",
    "               5:'Outlook', 6:'Skype', 7:'SMB', 8:'Weibo', 9:'WorldOfWarcraft',\n",
    "               10:'Cridex', 11:'Geodo', 12:'Htbot', 13:'Miuref', 14:'Neris',\n",
    "               15:'Nsis-ay', 16:'Shifu', 17:'Tinba', 18:'Virut', 19:'Zeus'}\n",
    "\n",
    "# --- Создание модели (максимально близко к оригиналу) ---\n",
    "def create_original_model(class_num, include_softmax=True):\n",
    "    \"\"\"\n",
    "    Создает модель CNN с использованием Keras Functional API,\n",
    "    ожидающую вход (28, 28, 1).\n",
    "\n",
    "    Args:\n",
    "        class_num (int): Количество выходных классов.\n",
    "        include_softmax (bool): Включать ли Softmax.\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: Скомпилированная модель Keras Functional API.\n",
    "    \"\"\"\n",
    "    # Определяем входной слой\n",
    "    model = models.Sequential(name=f\"SequentialOriginalLike_{class_num}class\")\n",
    "    # НЕ добавляем Input слой явно\n",
    "    # Вход плоский (784), Reshape внутри\n",
    "    model.add(layers.Reshape((28, 28, 1), input_shape=(784,))) # <--- ВАЖНО\n",
    "\n",
    "    # Сверточные слои\n",
    "    model.add(layers.Conv2D(32, (5, 5), padding='same', activation='relu', name='conv1'))\n",
    "    # Используем padding='valid' по умолчанию для MaxPooling, как было в рабочем коде неявно\n",
    "    model.add(layers.MaxPooling2D((2, 2), name='pool1')) # <--- padding='valid' по умолч.\n",
    "\n",
    "    model.add(layers.Conv2D(64, (5, 5), padding='same', activation='relu', name='conv2'))\n",
    "    model.add(layers.MaxPooling2D((2, 2), name='pool2')) # <--- padding='valid' по умолч.\n",
    "\n",
    "    # Полносвязные слои\n",
    "    model.add(layers.Flatten(name='flatten'))\n",
    "    model.add(layers.Dense(1024, activation='relu', name='dense1'))\n",
    "    model.add(layers.Dropout(0.5, name='dropout'))\n",
    "\n",
    "    # Выходной слой\n",
    "    if include_softmax:\n",
    "        model.add(layers.Dense(class_num, activation='softmax', name='output_softmax'))\n",
    "        loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
    "    else:\n",
    "        model.add(layers.Dense(class_num, name='output_logits'))\n",
    "        loss_function = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    # Создаем модель Functional API\n",
    "    # model = models.Model(inputs=input_tensor, outputs=output_tensor, name=f\"OriginalCNN_Functional_{class_num}class\")\n",
    "\n",
    "    # Компиляция модели\n",
    "    # optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "    model.compile(optimizer=optimizer, loss=loss_function, metrics=['accuracy'])\n",
    "    model.build(input_shape=(None, 784))\n",
    "\n",
    "    print(\"--- Модель (Functional API) создана ---\")\n",
    "    model.summary()\n",
    "    print(\"---------------------------------------\")\n",
    "    return model\n",
    "\n",
    "# --- Обучение модели ---\n",
    "def train_model(model, data_loader, train_steps, batch_size, model_save_path):\n",
    "    \"\"\"\n",
    "    Обучает модель с использованием model.fit и сохраняет лучшую версию.\n",
    "\n",
    "    Args:\n",
    "        model (tf.keras.Model): Скомпилированная модель Keras.\n",
    "        data_loader (MNISTDataLoader): Загрузчик данных.\n",
    "        train_steps (int): Общее количество шагов обучения (как TRAIN_ROUND).\n",
    "        batch_size (int): Размер батча.\n",
    "        model_save_path (str): Путь для сохранения лучшей модели (.h5).\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.callbacks.History: История обучения.\n",
    "    \"\"\"\n",
    "    # Рассчитываем количество эпох\n",
    "    steps_per_epoch = max(1, data_loader.num_train_examples // batch_size)\n",
    "    epochs = max(1, train_steps // steps_per_epoch)\n",
    "    print(f\"Расчетное количество эпох: {epochs} ({train_steps} шагов / {steps_per_epoch} шагов в эпохе)\")\n",
    "\n",
    "    # Создаем директорию для сохранения, если ее нет\n",
    "    os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n",
    "\n",
    "    # Колбэк для сохранения лучшей модели по val_accuracy\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        filepath=model_save_path,\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False, # Сохраняем всю модель\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    )\n",
    "    # Колбэк для ранней остановки, если улучшений нет\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=10, # Количество эпох без улучшений перед остановкой (настройте)\n",
    "        verbose=1,\n",
    "        restore_best_weights=True # Восстановить лучшие веса в конце\n",
    "    )\n",
    "\n",
    "    print(f\"Начало обучения на {epochs} эпох...\")\n",
    "    history = model.fit(\n",
    "        data_loader.train_images,\n",
    "        data_loader.train_labels,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=(data_loader.test_images, data_loader.test_labels),\n",
    "        callbacks=[checkpoint, early_stopping],\n",
    "        verbose=1 # Показываем прогресс\n",
    "    )\n",
    "    print(\"Обучение завершено.\")\n",
    "    # Модель уже содержит лучшие веса благодаря restore_best_weights=True\n",
    "    return history\n",
    "\n",
    "# --- Оценка модели ---\n",
    "def evaluate_model(model, data_loader, class_num, data_dir):\n",
    "    \"\"\"\n",
    "    Оценивает модель на тестовых данных и выводит метрики.\n",
    "\n",
    "    Args:\n",
    "        model (tf.keras.Model): Обученная модель Keras.\n",
    "        data_loader (MNISTDataLoader): Загрузчик данных.\n",
    "        class_num (int): Количество классов.\n",
    "        data_dir (str): Директория данных (для выбора словаря).\n",
    "    \"\"\"\n",
    "    print(\"\\nНачало оценки модели на тестовых данных...\")\n",
    "    y_true_one_hot = data_loader.test_labels\n",
    "    y_true = np.argmax(y_true_one_hot, axis=1)\n",
    "\n",
    "    # Получаем предсказания (логиты или вероятности, в зависимости от include_softmax)\n",
    "    predictions = model.predict(data_loader.test_images)\n",
    "\n",
    "    # Если модель выводит логиты, применяем argmax\n",
    "    # Если выводит вероятности (softmax), argmax тоже сработает\n",
    "    y_pred = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # Выбор словаря для имен классов\n",
    "    folder = os.path.basename(data_dir)\n",
    "    dict_labels = {}\n",
    "    if class_num == 2: dict_labels = dict_2class\n",
    "    elif class_num == 20: dict_labels = dict_20class\n",
    "    elif class_num == 10:\n",
    "        if folder.startswith('Benign'): dict_labels = dict_10class_benign\n",
    "        elif folder.startswith('Malware'): dict_labels = dict_10class_malware\n",
    "    label_names = [dict_labels.get(i, f'Class_{i}') for i in range(class_num)]\n",
    "\n",
    "    # Расчет метрик\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # average=None возвращает метрики для каждого класса\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=None, labels=list(range(class_num)), zero_division=0\n",
    "    )\n",
    "    # Средние метрики (можно использовать 'macro' или 'weighted')\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average='macro', zero_division=0\n",
    "    )\n",
    "\n",
    "    print(\"--- Результаты Оценки ---\")\n",
    "    print(f\"Общая точность (Accuracy): {accuracy:.4f}\")\n",
    "    print(f\"Precision (Macro Avg):    {precision_macro:.4f}\")\n",
    "    print(f\"Recall (Macro Avg):       {recall_macro:.4f}\")\n",
    "    print(f\"F1-Score (Macro Avg):     {f1_macro:.4f}\")\n",
    "    print(\"\\nМетрики по классам:\")\n",
    "    print(f\"{'Класс':<6} {'Имя':<18} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'Support':<8}\")\n",
    "    print(\"-\" * 70)\n",
    "    acc_list_str = []\n",
    "    for i in range(class_num):\n",
    "        name = label_names[i]\n",
    "        print(f\"{i:<6} {name:<18} {precision[i]:<10.4f} {recall[i]:<10.4f} {f1[i]:<10.4f} {support[i]:<8}\")\n",
    "        # Формируем строку для записи в файл (как в оригинале)\n",
    "        acc_list_str.append([str(i), name, f\"{precision[i]:.4f}\", f\"{recall[i]:.4f}\"])\n",
    "\n",
    "    # Запись в файл (добавляем в конец)\n",
    "    try:\n",
    "        with open('out_tf2.txt', 'a') as f:\n",
    "            f.write(\"\\n\")\n",
    "            t = time.strftime('%Y-%m-%d %X', time.localtime())\n",
    "            f.write(t + \"\\n\")\n",
    "            f.write(f'DATA_DIR: {data_dir}\\n')\n",
    "            f.write(f'CLASS_NUM: {class_num}\\n')\n",
    "            f.write(f'MODEL: Original TF2 Reimplementation\\n')\n",
    "            f.write(\"Класс, Имя, Precision, Recall\\n\")\n",
    "            for item in acc_list_str:\n",
    "                f.write(', '.join(item) + \"\\n\")\n",
    "            f.write(f'Total accuracy: {accuracy:.4f}\\n')\n",
    "            f.write(f'Precision (Macro): {precision_macro:.4f}\\n')\n",
    "            f.write(f'Recall (Macro): {recall_macro:.4f}\\n')\n",
    "            f.write(f'F1-Score (Macro): {f1_macro:.4f}\\n\\n')\n",
    "        print(\"\\nРезультаты оценки записаны в out_tf2.txt\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nОшибка записи результатов оценки в файл: {e}\")\n",
    "\n",
    "    print(\"-------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback # Добавим для вывода стека ошибки\n",
    "\n",
    "def export_to_onnx(model_to_convert, onnx_save_path):\n",
    "    \"\"\"\n",
    "    Экспортирует модель Keras в формат ONNX, используя Functional API wrapper.\n",
    "\n",
    "    Args:\n",
    "        model_to_convert (tf.keras.Model): Обученная модель Keras (Sequential или Functional).\n",
    "        onnx_save_path (str): Путь для сохранения ONNX модели.\n",
    "\n",
    "    Returns:\n",
    "        bool: True если экспорт успешен, иначе False.\n",
    "    \"\"\"\n",
    "    print(f\"\\nЭкспорт модели в ONNX: {onnx_save_path}\")\n",
    "    try:\n",
    "        # 1. Определяем входную сигнатуру (плоский вектор 784)\n",
    "        # Имя 'input_flat' должно совпадать с именем входного слоя модели или нового Input\n",
    "        input_signature = [tf.TensorSpec((None, 784), tf.float32, name='input_flat')]\n",
    "\n",
    "        # --- Создаем Functional API Wrapper ---\n",
    "        # Создаем явный входной слой с тем же shape и именем, что ожидает модель\n",
    "        func_input = tf.keras.Input(\n",
    "            shape=(784,), batch_size=None, dtype=tf.float32, name='input_flat'\n",
    "        )\n",
    "        # Пропускаем вход через существующую модель\n",
    "        # model_to_convert здесь - это ваша обученная Sequential модель\n",
    "        func_output = model_to_convert(func_input)\n",
    "        # Создаем новую модель-обертку Functional API\n",
    "        wrapper_model = tf.keras.Model(inputs=func_input, outputs=func_output, name=model_to_convert.name + \"_wrapper\")\n",
    "        print(\"--- Структура модели-обертки (для ONNX) ---\")\n",
    "        wrapper_model.summary()\n",
    "        print(\"-------------------------------------------\")\n",
    "        # --- Обертка создана ---\n",
    "\n",
    "        # 2. Конвертируем модель-ОБЕРТКУ\n",
    "        model_proto, external_tensor_storage = tf2onnx.convert.from_keras(\n",
    "            wrapper_model, # <--- Конвертируем обертку!\n",
    "            input_signature=input_signature,\n",
    "            opset=13,\n",
    "            output_path=onnx_save_path\n",
    "        )\n",
    "        print(f\"Модель успешно экспортирована в ONNX: {onnx_save_path}\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка экспорта в ONNX: {e}\")\n",
    "        print(\"--- Traceback ---\")\n",
    "        print(traceback.format_exc()) # Выводим полный traceback для диагностики\n",
    "        print(\"-----------------\")\n",
    "        return False\n",
    "    \n",
    "\n",
    "\n",
    "    # --- Проверка ONNX модели ---\n",
    "def test_onnx_model(onnx_path, data_loader):\n",
    "    \"\"\"\n",
    "    Загружает ONNX модель и выполняет инференс на нескольких примерах.\n",
    "\n",
    "    Args:\n",
    "        onnx_path (str): Путь к ONNX модели.\n",
    "        data_loader (MNISTDataLoader): Загрузчик данных для тестовых примеров.\n",
    "    \"\"\"\n",
    "    if not onnx_path or not os.path.exists(onnx_path):\n",
    "        print(\"Пропуск проверки ONNX: файл не найден.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nПроверка ONNX модели: {onnx_path}\")\n",
    "    try:\n",
    "        sess = ort.InferenceSession(onnx_path, providers=ort.get_available_providers()) # Используем доступные провайдеры\n",
    "        input_name = sess.get_inputs()[0].name\n",
    "        output_name = sess.get_outputs()[0].name\n",
    "        print(f\"  ONNX Input: '{input_name}', Output: '{output_name}'\")\n",
    "\n",
    "        # Берем несколько тестовых примеров\n",
    "        sample_images = data_loader.test_images[:5]\n",
    "        sample_labels_true = np.argmax(data_loader.test_labels[:5], axis=1)\n",
    "\n",
    "        # Выполняем инференс\n",
    "        outputs_onnx = sess.run([output_name], {input_name: sample_images.astype(np.float32)})[0]\n",
    "        predictions_onnx = np.argmax(outputs_onnx, axis=1)\n",
    "\n",
    "        print(\"  Примеры предсказаний ONNX:\")\n",
    "        for i in range(len(sample_images)):\n",
    "            print(f\"    Пример {i+1}: Предсказано={predictions_onnx[i]}, Истина={sample_labels_true[i]}\")\n",
    "\n",
    "        # Сравним с Keras моделью (если она доступна глобально - не лучший стиль, но для примера)\n",
    "        # predictions_keras = np.argmax(model.predict(sample_images), axis=1)\n",
    "        # print(f\"  Совпадают ли предсказания Keras и ONNX: {np.array_equal(predictions_keras, predictions_onnx)}\")\n",
    "\n",
    "        print(\"Проверка ONNX модели завершена.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка проверки ONNX модели: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Параметры Запуска ---\n",
      "DATA_DIR:          ./dataset/20class/SessionAllLayers\n",
      "CLASS_NUM:         20\n",
      "TRAIN_STEPS:       4000\n",
      "BATCH_SIZE:        50\n",
      "MODEL_SAVE_DIR:    ./trained_models_colab\n",
      "LOAD_WEIGHTS_PATH: False\n",
      "SKIP_TRAIN:        False\n",
      "EXPORT_ONNX:       True\n",
      "TEST_ONNX:         True\n",
      "NO_SOFTMAX:        True\n",
      "-------------------------\n",
      "Загрузка данных из: ./dataset/20class/SessionAllLayers\n",
      "Данные загружены.\n",
      "  Train: (128433, 784), (128433, 20)\n",
      "  Test:  (14267, 784), (14267, 20)\n",
      "--- Модель (Functional API) создана ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/n1sko/tf_venv/lib/python3.10/site-packages/keras/src/layers/reshaping/reshape.py:39: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"SequentialOriginalLike_20class\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"SequentialOriginalLike_20class\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ reshape_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">832</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ pool1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">51,264</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ pool2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3136</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,212,288</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output_logits (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">20,500</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ reshape_3 (\u001b[38;5;33mReshape\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1 (\u001b[38;5;33mConv2D\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m832\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ pool1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2 (\u001b[38;5;33mConv2D\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m51,264\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ pool2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3136\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense1 (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │     \u001b[38;5;34m3,212,288\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output_logits (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │        \u001b[38;5;34m20,500\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,284,884</span> (12.53 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,284,884\u001b[0m (12.53 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,284,884</span> (12.53 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,284,884\u001b[0m (12.53 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Расчетное количество эпох: 1 (4000 шагов / 2568 шагов в эпохе)\n",
      "Начало обучения на 1 эпох...\n",
      "\u001b[1m2569/2569\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6949 - loss: 0.9979\n",
      "Epoch 1: val_accuracy improved from -inf to 0.93299, saving model to ./trained_models_colab/model_20class_SessionAllLayers_logits/model_20class_SessionAllLayers_logits.keras\n",
      "\u001b[1m2569/2569\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 0.6949 - loss: 0.9977 - val_accuracy: 0.9330 - val_loss: 0.1683\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Обучение завершено.\n",
      "Лучшая модель сохранена в: ./trained_models_colab/model_20class_SessionAllLayers_logits/model_20class_SessionAllLayers_logits.keras\n",
      "Загрузка лучшей сохраненной модели для оценки/экспорта...\n",
      "Лучшая модель загружена.\n",
      "\n",
      "Начало оценки модели на тестовых данных...\n",
      "\u001b[1m446/446\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--- Результаты Оценки ---\n",
      "Общая точность (Accuracy): 0.9330\n",
      "Precision (Macro Avg):    0.9444\n",
      "Recall (Macro Avg):       0.9347\n",
      "F1-Score (Macro Avg):     0.9363\n",
      "\n",
      "Метрики по классам:\n",
      "Класс  Имя                Precision  Recall     F1-Score   Support \n",
      "----------------------------------------------------------------------\n",
      "0      BitTorrent         0.9266     0.5878     0.7193     752     \n",
      "1      Facetime           1.0000     0.9833     0.9916     600     \n",
      "2      FTP                1.0000     1.0000     1.0000     689     \n",
      "3      Gmail              0.6492     0.8899     0.7507     863     \n",
      "4      MySQL              1.0000     1.0000     1.0000     724     \n",
      "5      Outlook            0.8494     0.7952     0.8214     752     \n",
      "6      Skype              0.9814     1.0000     0.9906     632     \n",
      "7      SMB                0.9834     0.9384     0.9604     633     \n",
      "8      Weibo              0.9427     0.9865     0.9641     667     \n",
      "9      WorldOfWarcraft    1.0000     0.9962     0.9981     788     \n",
      "10     Cridex             1.0000     0.9927     0.9963     822     \n",
      "11     Geodo              0.9955     0.9910     0.9933     670     \n",
      "12     Htbot              0.9748     0.9717     0.9733     637     \n",
      "13     Miuref             0.9790     1.0000     0.9894     513     \n",
      "14     Neris              0.7992     0.9035     0.8482     850     \n",
      "15     Nsis-ay            0.9845     0.9407     0.9621     607     \n",
      "16     Shifu              0.9948     0.9907     0.9927     963     \n",
      "17     Tinba              0.9930     0.9965     0.9947     850     \n",
      "18     Virut              0.8362     0.7362     0.7830     652     \n",
      "19     Zeus               0.9983     0.9934     0.9958     603     \n",
      "\n",
      "Результаты оценки записаны в out_tf2.txt\n",
      "-------------------------\n",
      "\n",
      "Экспорт модели в ONNX: ./trained_models_colab/model_20class_SessionAllLayers_logits/model_20class_SessionAllLayers_logits.onnx\n",
      "--- Структура модели-обертки (для ONNX) ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"SequentialOriginalLike_20class_wrapper\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"SequentialOriginalLike_20class_wrapper\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_flat (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ SequentialOriginalLike_20class  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,284,884</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)                    │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_flat (\u001b[38;5;33mInputLayer\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ SequentialOriginalLike_20class  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │     \u001b[38;5;34m3,284,884\u001b[0m │\n",
       "│ (\u001b[38;5;33mSequential\u001b[0m)                    │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,284,884</span> (12.53 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,284,884\u001b[0m (12.53 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,284,884</span> (12.53 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,284,884\u001b[0m (12.53 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1743734386.040899  159673 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\n",
      "I0000 00:00:1743734386.041079  159673 single_machine.cc:374] Starting new session\n",
      "I0000 00:00:1743734386.041623  159673 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13553 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Ti SUPER, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "I0000 00:00:1743734386.128475  159673 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13553 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Ti SUPER, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "I0000 00:00:1743734386.148072  159673 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\n",
      "I0000 00:00:1743734386.148244  159673 single_machine.cc:374] Starting new session\n",
      "I0000 00:00:1743734386.148664  159673 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13553 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Ti SUPER, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "rewriter <function rewrite_constant_fold at 0x7fd396401a20>: exception `np.cast` was removed in the NumPy 2.0 release. Use `np.asarray(arr, dtype=dtype)` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель успешно экспортирована в ONNX: ./trained_models_colab/model_20class_SessionAllLayers_logits/model_20class_SessionAllLayers_logits.onnx\n",
      "\n",
      "Проверка ONNX модели: ./trained_models_colab/model_20class_SessionAllLayers_logits/model_20class_SessionAllLayers_logits.onnx\n",
      "*************** EP Error ***************\n",
      "EP Error /onnxruntime_src/onnxruntime/python/onnxruntime_pybind_state.cc:505 void onnxruntime::python::RegisterTensorRTPluginsAsCustomOps(PySessionOptions&, const onnxruntime::ProviderOptions&) Please install TensorRT libraries as mentioned in the GPU requirements page, make sure they're in the PATH or LD_LIBRARY_PATH, and that your GPU is supported.\n",
      " when using ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']\n",
      "Falling back to ['CUDAExecutionProvider', 'CPUExecutionProvider'] and retrying.\n",
      "****************************************\n",
      "  ONNX Input: 'input_flat', Output: 'SequentialOriginalLike_20class'\n",
      "  Примеры предсказаний ONNX:\n",
      "    Пример 1: Предсказано=10, Истина=10\n",
      "    Пример 2: Предсказано=17, Истина=17\n",
      "    Пример 3: Предсказано=12, Истина=12\n",
      "    Пример 4: Предсказано=12, Истина=12\n",
      "    Пример 5: Предсказано=18, Истина=18\n",
      "Проверка ONNX модели завершена.\n",
      "\n",
      "Скрипт завершен.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;31m2025-04-04 09:39:46.352275331 [E:onnxruntime:Default, provider_bridge_ort.cc:2022 TryGetProviderInfo_TensorRT] /onnxruntime_src/onnxruntime/core/session/provider_bridge_ort.cc:1695 onnxruntime::Provider& onnxruntime::ProviderLibrary::Get() [ONNXRuntimeError] : 1 : FAIL : Failed to load library libonnxruntime_providers_tensorrt.so with error: libnvinfer.so.10: cannot open shared object file: No such file or directory\n",
      "\u001b[m\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # --- Параметры ---\n",
    "    # Задайте путь к вашим данным на Google Drive\n",
    "    # Убедитесь, что ваш Google Drive смонтирован в Colab\n",
    "    # Пример: from google.colab import drive; drive.mount('/content/drive')\n",
    "    DATA_DIR = \"./dataset/20class/SessionAllLayers\"\n",
    "\n",
    "    CLASS_NUM = 20              # Количество классов\n",
    "    TRAIN_STEPS = 4000         # Общее количество шагов обучения (TRAIN_ROUND)\n",
    "    BATCH_SIZE = 50             # Размер батча\n",
    "    MODEL_SAVE_DIR = './trained_models_colab' # Директория для сохранения моделей в Colab\n",
    "    LOAD_WEIGHTS_PATH = False #\"./trained_models_colab/model_20class_SessionAllLayers_logits/model_20class_SessionAllLayers_logits.keras\"    # Укажите путь к .h5 файлу, если хотите загрузить веса, например: '/content/drive/MyDrive/мои_веса.h5'\n",
    "    SKIP_TRAIN = False          # Установите True, если хотите пропустить обучение (требует LOAD_WEIGHTS_PATH)\n",
    "    EXPORT_ONNX = True          # Экспортировать ли модель в ONNX\n",
    "    TEST_ONNX = True            # Проверять ли ONNX модель (требует EXPORT_ONNX)\n",
    "    NO_SOFTMAX = True           # Создать модель БЕЗ финального Softmax (Рекомендуется для NMDL)\n",
    "    # ------------------\n",
    "\n",
    "    # Проверка аргументов (логика из парсера)\n",
    "    if SKIP_TRAIN and not LOAD_WEIGHTS_PATH:\n",
    "        print(\"Ошибка: SKIP_TRAIN установлен в True, но LOAD_WEIGHTS_PATH не указан.\")\n",
    "        sys.exit(1)\n",
    "    if TEST_ONNX and not EXPORT_ONNX:\n",
    "        print(\"Предупреждение: TEST_ONNX установлен в True, но EXPORT_ONNX в False. Экспорт будет выполнен.\")\n",
    "        EXPORT_ONNX = True\n",
    "\n",
    "    print(\"--- Параметры Запуска ---\")\n",
    "    print(f\"DATA_DIR:          {DATA_DIR}\")\n",
    "    print(f\"CLASS_NUM:         {CLASS_NUM}\")\n",
    "    print(f\"TRAIN_STEPS:       {TRAIN_STEPS}\")\n",
    "    print(f\"BATCH_SIZE:        {BATCH_SIZE}\")\n",
    "    print(f\"MODEL_SAVE_DIR:    {MODEL_SAVE_DIR}\")\n",
    "    print(f\"LOAD_WEIGHTS_PATH: {LOAD_WEIGHTS_PATH}\")\n",
    "    print(f\"SKIP_TRAIN:        {SKIP_TRAIN}\")\n",
    "    print(f\"EXPORT_ONNX:       {EXPORT_ONNX}\")\n",
    "    print(f\"TEST_ONNX:         {TEST_ONNX}\")\n",
    "    print(f\"NO_SOFTMAX:        {NO_SOFTMAX}\")\n",
    "    print(\"-------------------------\")\n",
    "\n",
    "    # Загрузка данных\n",
    "    try:\n",
    "        data_loader = MNISTDataLoader(DATA_DIR, one_hot=True, class_num=CLASS_NUM)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Ошибка: Директория данных или файлы не найдены! {e}\")\n",
    "        sys.exit(1)\n",
    "    except Exception as e:\n",
    "         print(f\"Непредвиденная ошибка при загрузке данных: {e}\")\n",
    "         sys.exit(1)\n",
    "\n",
    "    # Определяем пути для сохранения\n",
    "    folder_name = os.path.basename(DATA_DIR.rstrip('/')) or f\"data_{CLASS_NUM}class\" # Убираем слэш в конце если есть\n",
    "    model_base_name = f\"model_{CLASS_NUM}class_{folder_name}\"\n",
    "    if NO_SOFTMAX:\n",
    "        model_base_name += \"_logits\"\n",
    "    # Убедимся, что основная директория для сохранения существует\n",
    "    os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "    model_save_sub_dir = os.path.join(MODEL_SAVE_DIR, model_base_name) # Поддиректория для конкретной модели\n",
    "    model_save_path = os.path.join(model_save_sub_dir, model_base_name + \".keras\")\n",
    "    onnx_save_path = os.path.join(model_save_sub_dir, model_base_name + \".onnx\")\n",
    "\n",
    "    # Создание модели\n",
    "    model = create_original_model(CLASS_NUM, include_softmax=(not NO_SOFTMAX))\n",
    "\n",
    "    # Обучение или загрузка весов\n",
    "    if not SKIP_TRAIN:\n",
    "        if LOAD_WEIGHTS_PATH:\n",
    "            if os.path.exists(LOAD_WEIGHTS_PATH):\n",
    "                try:\n",
    "                    print(f\"Загрузка весов из: {LOAD_WEIGHTS_PATH}\")\n",
    "                    # Загружаем только веса, предполагая совпадение архитектуры\n",
    "                    model.load_weights(LOAD_WEIGHTS_PATH)\n",
    "                    print(\"Веса загружены успешно.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Ошибка загрузки весов из {LOAD_WEIGHTS_PATH}: {e}\")\n",
    "                    print(\"Начинаем обучение с нуля.\")\n",
    "                    # Переходим к обучению, т.к. загрузка не удалась\n",
    "                    train_model(model, data_loader, TRAIN_STEPS, BATCH_SIZE, model_save_path)\n",
    "            else:\n",
    "                print(f\"Предупреждение: Файл весов {LOAD_WEIGHTS_PATH} не найден. Начинаем обучение с нуля.\")\n",
    "                train_model(model, data_loader, TRAIN_STEPS, BATCH_SIZE, model_save_path)\n",
    "        else:\n",
    "            # Обучаем с нуля\n",
    "            train_model(model, data_loader, TRAIN_STEPS, BATCH_SIZE, model_save_path)\n",
    "            print(f\"Лучшая модель сохранена в: {model_save_path}\")\n",
    "            # Загружаем лучшую модель после обучения для последующих шагов\n",
    "            print(\"Загрузка лучшей сохраненной модели для оценки/экспорта...\")\n",
    "            try:\n",
    "                # Перезагружаем модель, которая была сохранена колбэком\n",
    "                # Компиляция сохранится, если save_weights_only=False (по умолчанию)\n",
    "                model = tf.keras.models.load_model(model_save_path)\n",
    "                print(\"Лучшая модель загружена.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Ошибка загрузки лучшей модели из {model_save_path}: {e}\")\n",
    "                print(\"Оценка и экспорт будут выполнены с моделью в текущем состоянии.\")\n",
    "\n",
    "\n",
    "    elif LOAD_WEIGHTS_PATH: # Если пропустили трейн, но указали веса\n",
    "         if os.path.exists(LOAD_WEIGHTS_PATH):\n",
    "             try:\n",
    "                 print(f\"Загрузка весов из: {LOAD_WEIGHTS_PATH}\")\n",
    "                 # Загружаем только веса в созданную архитектуру\n",
    "                 model.load_weights(LOAD_WEIGHTS_PATH)\n",
    "                 print(\"Веса загружены успешно.\")\n",
    "             except Exception as e:\n",
    "                 print(f\"Ошибка загрузки весов из {LOAD_WEIGHTS_PATH}: {e}\")\n",
    "                 sys.exit(1)\n",
    "         else:\n",
    "             print(f\"Ошибка: Файл весов {LOAD_WEIGHTS_PATH} не найден, обучение пропущено.\")\n",
    "             sys.exit(1)\n",
    "    else:\n",
    "        print(\"Ошибка: Обучение пропущено (SKIP_TRAIN=True), но не указан путь для загрузки весов (LOAD_WEIGHTS_PATH).\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Оценка модели (уже обученной или с загруженными весами)\n",
    "    if model: # Убедимся, что модель существует\n",
    "        evaluate_model(model, data_loader, CLASS_NUM, DATA_DIR)\n",
    "    else:\n",
    "        print(\"Ошибка: Модель не была обучена или загружена. Пропуск оценки.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "    # Экспорт в ONNX\n",
    "    onnx_exported = False\n",
    "    if EXPORT_ONNX and model:\n",
    "       os.makedirs(model_save_sub_dir, exist_ok=True) # Убедимся что директория есть\n",
    "       onnx_exported = export_to_onnx(model, onnx_save_path)\n",
    "\n",
    "    # Проверка ONNX\n",
    "    if TEST_ONNX and onnx_exported:\n",
    "        test_onnx_model(onnx_save_path, data_loader)\n",
    "    elif TEST_ONNX and not onnx_exported:\n",
    "        print(\"Пропуск проверки ONNX: экспорт не удался или был пропущен.\")\n",
    "\n",
    "    print(\"\\nСкрипт завершен.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
