{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "import os\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Input\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import gzip\n",
    "import onnx\n",
    "import tf2onnx\n",
    "import onnxruntime as ort # Для проверки ONNX\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "# --- Класс загрузки данных (ваш класс) ---\n",
    "class MNISTDataLoader:\n",
    "    def __init__(self, data_dir, one_hot=True, class_num=10):\n",
    "        self.data_dir = data_dir\n",
    "        self.one_hot = one_hot\n",
    "        self.class_num = class_num\n",
    "\n",
    "        # Проверяем существование директории\n",
    "        if not os.path.isdir(self.data_dir):\n",
    "            raise FileNotFoundError(f\"Директория данных не найдена: {self.data_dir}\")\n",
    "\n",
    "        print(f\"Загрузка данных из: {self.data_dir}\")\n",
    "        self.train_images, self.train_labels = self._load_data('train')\n",
    "        self.test_images, self.test_labels = self._load_data('t10k')\n",
    "        print(\"Данные загружены.\")\n",
    "        print(f\"  Train: {self.train_images.shape}, {self.train_labels.shape}\")\n",
    "        print(f\"  Test:  {self.test_images.shape}, {self.test_labels.shape}\")\n",
    "\n",
    "        # Добавляем свойство для количества тренировочных примеров\n",
    "        self.num_train_examples = len(self.train_images)\n",
    "\n",
    "    def _load_data(self, prefix):\n",
    "        \"\"\"Load MNIST data from .gz files\"\"\"\n",
    "        image_path = os.path.join(self.data_dir, f'{prefix}-images-idx3-ubyte.gz')\n",
    "        label_path = os.path.join(self.data_dir, f'{prefix}-labels-idx1-ubyte.gz')\n",
    "\n",
    "        if not os.path.exists(image_path) or not os.path.exists(label_path):\n",
    "             raise FileNotFoundError(f\"Файлы данных MNIST не найдены в {self.data_dir} с префиксом '{prefix}'\")\n",
    "\n",
    "        try:\n",
    "            # Load images\n",
    "            with gzip.open(image_path, 'rb') as f:\n",
    "                # Смещение 16 байт для заголовка IDX3\n",
    "                images = np.frombuffer(f.read(), np.uint8, offset=16)\n",
    "                # MNIST изображения 28x28 = 784\n",
    "                images = images.reshape(-1, 784).astype(np.float32) / 255.0\n",
    "                # images = images.reshape(-1, 28, 28, 1).astype(np.float32) / 255.0 # <--- ИЗМЕНЕНО ЗДЕСЬ\n",
    "\n",
    "\n",
    "            # Load labels\n",
    "            with gzip.open(label_path, 'rb') as f:\n",
    "                # Смещение 8 байт для заголовка IDX1\n",
    "                labels = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "\n",
    "            # Convert to one-hot if needed\n",
    "            if self.one_hot:\n",
    "                # Используем tf.keras.utils.to_categorical для надежности\n",
    "                one_hot_labels = tf.keras.utils.to_categorical(labels, num_classes=self.class_num)\n",
    "                # labels = tf.one_hot(labels, self.class_num).numpy() # Альтернатива\n",
    "                return images, one_hot_labels\n",
    "\n",
    "            return images, labels\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка при загрузке или обработке файлов {prefix}: {e}\")\n",
    "            raise\n",
    "\n",
    "# --- Словари классов (как в оригинале) ---\n",
    "dict_2class = {0:'Benign',1:'Malware'}\n",
    "dict_10class_benign = {0:'BitTorrent',1:'Facetime',2:'FTP',3:'Gmail',4:'MySQL',5:'Outlook',6:'Skype',7:'SMB',8:'Weibo',9:'WorldOfWarcraft'}\n",
    "dict_10class_malware = {0:'Cridex',1:'Geodo',2:'Htbot',3:'Miuref',4:'Neris',5:'Nsis-ay',6:'Shifu',7:'Tinba',8:'Virut',9:'Zeus'}\n",
    "dict_20class = {0:'BitTorrent', 1:'Facetime', 2:'FTP', 3:'Gmail', 4:'MySQL',\n",
    "               5:'Outlook', 6:'Skype', 7:'SMB', 8:'Weibo', 9:'WorldOfWarcraft',\n",
    "               10:'Cridex', 11:'Geodo', 12:'Htbot', 13:'Miuref', 14:'Neris',\n",
    "               15:'Nsis-ay', 16:'Shifu', 17:'Tinba', 18:'Virut', 19:'Zeus'}\n",
    "\n",
    "# --- Создание модели (максимально близко к оригиналу) ---\n",
    "def create_original_model(class_num: int, include_softmax: bool = True) -> tf.keras.Model:\n",
    "    # Входной слой (784,) -> Reshape -> (28, 28, 1)\n",
    "    input_tensor = tf.keras.Input(shape=(784,), name=\"input\")\n",
    "    x = layers.Reshape((28, 28, 1), name=\"reshape\")(input_tensor)\n",
    "\n",
    "    # Сверточные слои\n",
    "    x = layers.Conv2D(32, (5, 5), padding='same', activation='relu', name='conv1')(x)\n",
    "    x = layers.MaxPooling2D((2, 2), name='pool1')(x)\n",
    "\n",
    "    x = layers.Conv2D(64, (5, 5), padding='same', activation='relu', name='conv2')(x)\n",
    "    x = layers.MaxPooling2D((2, 2), name='pool2')(x)\n",
    "\n",
    "    # Полносвязные слои\n",
    "    x = layers.Flatten(name='flatten')(x)\n",
    "    x = layers.Dense(1024, activation='relu', name='dense1')(x)\n",
    "    x = layers.Dropout(0.5, name='dropout')(x)\n",
    "\n",
    "    # Выходной слой\n",
    "    if include_softmax:\n",
    "        output_tensor = layers.Dense(class_num, activation='softmax', name=\"output_softmax\")(x)\n",
    "        loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
    "    else:\n",
    "        output_tensor = layers.Dense(class_num, name=\"output_logits\")(x)\n",
    "        loss_function = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    # Финальная модель\n",
    "    model = tf.keras.Model(inputs=input_tensor, outputs=output_tensor, name=f\"OriginalCNN_{class_num}class\")\n",
    "\n",
    "    # Компиляция модели\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "    model.compile(optimizer=optimizer, loss=loss_function, metrics=['accuracy'])\n",
    "\n",
    "    # Показываем инфу\n",
    "    print(\"--- Модель (Functional API) создана ---\")\n",
    "    model.summary()\n",
    "    print(\"---------------------------------------\")\n",
    "    return model\n",
    "\n",
    "# --- Обучение модели ---\n",
    "def train_model(model, data_loader, train_steps, batch_size, model_save_path):\n",
    "    steps_per_epoch = max(1, data_loader.num_train_examples // batch_size)\n",
    "    epochs = max(1, train_steps // steps_per_epoch)\n",
    "    print(f\"Расчетное количество эпох: {epochs} ({train_steps} шагов / {steps_per_epoch} шагов в эпохе)\")\n",
    "\n",
    "    os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n",
    "\n",
    "    # Колбэк для сохранения лучшей модели по val_accuracy\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        filepath=model_save_path,\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False, # Сохраняем всю модель\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    )\n",
    "    # Колбэк для ранней остановки, если улучшений нет\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=3, # Количество эпох без улучшений перед остановкой\n",
    "        verbose=1,\n",
    "        restore_best_weights=True # Восстановить лучшие веса в конце\n",
    "    )\n",
    "\n",
    "    print(f\"Начало обучения на {epochs} эпох...\")\n",
    "    history = model.fit(\n",
    "        data_loader.train_images,\n",
    "        data_loader.train_labels,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=(data_loader.test_images, data_loader.test_labels),\n",
    "        callbacks=[checkpoint, early_stopping],\n",
    "        verbose=1 # Показываем прогресс\n",
    "    )\n",
    "    print(\"Обучение завершено.\")\n",
    "    return history\n",
    "\n",
    "# --- Оценка модели ---\n",
    "def evaluate_model(model, data_loader, class_num, data_dir):\n",
    "    \"\"\"\n",
    "    Оценивает модель на тестовых данных и выводит метрики.\n",
    "\n",
    "    Args:\n",
    "        model (tf.keras.Model): Обученная модель Keras.\n",
    "        data_loader (MNISTDataLoader): Загрузчик данных.\n",
    "        class_num (int): Количество классов.\n",
    "        data_dir (str): Директория данных (для выбора словаря).\n",
    "    \"\"\"\n",
    "    print(\"\\nНачало оценки модели на тестовых данных...\")\n",
    "    y_true_one_hot = data_loader.test_labels\n",
    "    y_true = np.argmax(y_true_one_hot, axis=1)\n",
    "\n",
    "    # Получаем предсказания (логиты или вероятности, в зависимости от include_softmax)\n",
    "    predictions = model.predict(data_loader.test_images)\n",
    "\n",
    "    # Если модель выводит логиты, применяем argmax\n",
    "    # Если выводит вероятности (softmax), argmax тоже сработает\n",
    "    y_pred = np.argmax(predictions, axis=1)\n",
    "\n",
    "    folder = os.path.basename(data_dir)\n",
    "    dict_labels = {}\n",
    "    if class_num == 2: dict_labels = dict_2class\n",
    "    elif class_num == 20: dict_labels = dict_20class\n",
    "    elif class_num == 10:\n",
    "        if folder.startswith('Benign'): dict_labels = dict_10class_benign\n",
    "        elif folder.startswith('Malware'): dict_labels = dict_10class_malware\n",
    "    label_names = [dict_labels.get(i, f'Class_{i}') for i in range(class_num)]\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # average=None возвращает метрики для каждого класса\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=None, labels=list(range(class_num)), zero_division=0\n",
    "    )\n",
    "    # Средние метрики (можно использовать 'macro' или 'weighted')\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average='macro', zero_division=0\n",
    "    )\n",
    "\n",
    "    print(\"--- Результаты Оценки ---\")\n",
    "    print(f\"Общая точность (Accuracy): {accuracy:.4f}\")\n",
    "    print(f\"Precision (Macro Avg):    {precision_macro:.4f}\")\n",
    "    print(f\"Recall (Macro Avg):       {recall_macro:.4f}\")\n",
    "    print(f\"F1-Score (Macro Avg):     {f1_macro:.4f}\")\n",
    "    print(\"\\nМетрики по классам:\")\n",
    "    print(f\"{'Класс':<6} {'Имя':<18} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'Support':<8}\")\n",
    "    print(\"-\" * 70)\n",
    "    acc_list_str = []\n",
    "    for i in range(class_num):\n",
    "        name = label_names[i]\n",
    "        print(f\"{i:<6} {name:<18} {precision[i]:<10.4f} {recall[i]:<10.4f} {f1[i]:<10.4f} {support[i]:<8}\")\n",
    "        # Формируем строку для записи в файл (как в оригинале)\n",
    "        acc_list_str.append([str(i), name, f\"{precision[i]:.4f}\", f\"{recall[i]:.4f}\"])\n",
    "\n",
    "    # Запись в файл (добавляем в конец)\n",
    "    try:\n",
    "        with open('out_tf2.txt', 'a') as f:\n",
    "            f.write(\"\\n\")\n",
    "            t = time.strftime('%Y-%m-%d %X', time.localtime())\n",
    "            f.write(t + \"\\n\")\n",
    "            f.write(f'DATA_DIR: {data_dir}\\n')\n",
    "            f.write(f'CLASS_NUM: {class_num}\\n')\n",
    "            f.write(f'MODEL: Original TF2 Reimplementation\\n')\n",
    "            f.write(\"Класс, Имя, Precision, Recall\\n\")\n",
    "            for item in acc_list_str:\n",
    "                f.write(', '.join(item) + \"\\n\")\n",
    "            f.write(f'Total accuracy: {accuracy:.4f}\\n')\n",
    "            f.write(f'Precision (Macro): {precision_macro:.4f}\\n')\n",
    "            f.write(f'Recall (Macro): {recall_macro:.4f}\\n')\n",
    "            f.write(f'F1-Score (Macro): {f1_macro:.4f}\\n\\n')\n",
    "        print(\"\\nРезультаты оценки записаны в out_tf2.txt\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nОшибка записи результатов оценки в файл: {e}\")\n",
    "\n",
    "    print(\"-------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tf2onnx\n",
    "import onnx\n",
    "from onnx.tools import update_model_dims # Инструмент для изменения размерностей\n",
    "import tensorflow as tf # Нужно для tf.TensorSpec\n",
    "import os\n",
    "import tempfile # Для временного файла\n",
    "import traceback\n",
    "\n",
    "def convert_to_onnx(model: tf.keras.Model, onnx_path: str):\n",
    "    \"\"\"\n",
    "    Конвертирует модель Keras в ONNX с фиксированным входом (1, 784).\n",
    "\n",
    "    Args:\n",
    "        model (tf.keras.Model): Обученная модель Keras.\n",
    "        onnx_path (str): Путь для сохранения ONNX-модели.\n",
    "    \"\"\"\n",
    "    import tf2onnx\n",
    "    import onnx\n",
    "\n",
    "    # Задаем фиксированный input shape (1, 784)\n",
    "    spec = (tf.TensorSpec((1, 784), tf.float32, name=\"input\"),)\n",
    "\n",
    "    print(f\"Конвертация модели в ONNX по пути: {onnx_path}\")\n",
    "    model_proto, _ = tf2onnx.convert.from_keras(\n",
    "        model,\n",
    "        input_signature=spec,\n",
    "        opset=13,\n",
    "        output_path=onnx_path\n",
    "    )\n",
    "    print(\"ONNX-модель успешно сохранена.\")\n",
    "\n",
    "\n",
    "\n",
    "# --- Обновите функцию test_onnx_model, если нужно ---\n",
    "# Функция test_onnx_model должна работать без изменений,\n",
    "# так как onnxruntime может обрабатывать модели с фиксированным размером батча=1,\n",
    "# просто нужно подавать на вход данные с батчем=1.\n",
    "\n",
    "def test_onnx_model(onnx_path, data_loader):\n",
    "    \"\"\"\n",
    "    Загружает ONNX модель и выполняет инференс на нескольких примерах.\n",
    "    (Работает с моделями, имеющими фиксированный размер батча = 1)\n",
    "    \"\"\"\n",
    "    if not onnx_path or not os.path.exists(onnx_path):\n",
    "        print(\"Пропуск проверки ONNX: файл не найден.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nПроверка ONNX модели с фиксированным батчем: {onnx_path}\")\n",
    "    try:\n",
    "        sess = ort.InferenceSession(onnx_path, providers=ort.get_available_providers())\n",
    "        input_name = sess.get_inputs()[0].name\n",
    "        output_name = sess.get_outputs()[0].name\n",
    "        input_shape = sess.get_inputs()[0].shape # Должно быть [1, 784]\n",
    "        output_shape = sess.get_outputs()[0].shape # Должно быть [1, class_num]\n",
    "        print(f\"  ONNX Input: '{input_name}' Shape: {input_shape}, Output: '{output_name}' Shape: {output_shape}\")\n",
    "\n",
    "        # Проверяем, что размер батча действительно 1\n",
    "        if input_shape[0] != 1 or output_shape[0] != 1:\n",
    "             print(f\"Предупреждение: Размер батча в ONNX модели не равен 1 (вход: {input_shape[0]}, выход: {output_shape[0]}). Фиксация могла не сработать.\")\n",
    "\n",
    "        # Берем несколько тестовых примеров\n",
    "        num_samples = 5\n",
    "        sample_images_batch = data_loader.test_images[:num_samples]\n",
    "        sample_labels_true = np.argmax(data_loader.test_labels[:num_samples], axis=1)\n",
    "\n",
    "        print(\"  Примеры предсказаний ONNX (по одному):\")\n",
    "        predictions_onnx = []\n",
    "        for i in range(num_samples):\n",
    "            # Берем один пример и добавляем измерение батча = 1\n",
    "            single_image = np.expand_dims(sample_images_batch[i], axis=0).astype(np.float32)\n",
    "            # Выполняем инференс для одного примера\n",
    "            output_single = sess.run([output_name], {input_name: single_image})[0]\n",
    "            # Получаем предсказание для этого примера\n",
    "            prediction_single = np.argmax(output_single, axis=1)[0] # [0] чтобы извлечь значение из массива размером 1\n",
    "            predictions_onnx.append(prediction_single)\n",
    "            print(f\"    Пример {i+1}: Предсказано={prediction_single}, Истина={sample_labels_true[i]}\")\n",
    "\n",
    "        print(\"Проверка ONNX модели завершена.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка проверки ONNX модели: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Параметры Запуска ---\n",
      "DATA_DIR:          ./dataset/20class/SessionAllLayers\n",
      "CLASS_NUM:         20\n",
      "TRAIN_STEPS:       2000\n",
      "BATCH_SIZE:        50\n",
      "MODEL_SAVE_DIR:    ./trained_models_colab\n",
      "LOAD_WEIGHTS_PATH: False\n",
      "SKIP_TRAIN:        False\n",
      "EXPORT_ONNX:       True\n",
      "TEST_ONNX:         True\n",
      "NO_SOFTMAX:        False\n",
      "-------------------------\n",
      "Загрузка данных из: ./dataset/20class/SessionAllLayers\n",
      "Данные загружены.\n",
      "  Train: (128433, 784), (128433, 20)\n",
      "  Test:  (14267, 784), (14267, 20)\n",
      "--- Модель (Functional API) создана ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"OriginalCNN_20class\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"OriginalCNN_20class\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">832</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ pool1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">51,264</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ pool2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3136</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,212,288</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output_softmax (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">20,500</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input (\u001b[38;5;33mInputLayer\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape (\u001b[38;5;33mReshape\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1 (\u001b[38;5;33mConv2D\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m832\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ pool1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2 (\u001b[38;5;33mConv2D\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m51,264\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ pool2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3136\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense1 (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │     \u001b[38;5;34m3,212,288\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output_softmax (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │        \u001b[38;5;34m20,500\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,284,884</span> (12.53 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,284,884\u001b[0m (12.53 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,284,884</span> (12.53 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,284,884\u001b[0m (12.53 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Расчетное количество эпох: 1 (2000 шагов / 2568 шагов в эпохе)\n",
      "Начало обучения на 1 эпох...\n",
      "\u001b[1m2569/2569\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7042 - loss: 0.9809\n",
      "Epoch 1: val_accuracy improved from -inf to 0.94435, saving model to ./trained_models_colab/model_20class_SessionAllLayers/model_20class_SessionAllLayers.keras\n",
      "\u001b[1m2569/2569\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 0.7043 - loss: 0.9807 - val_accuracy: 0.9443 - val_loss: 0.1660\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Обучение завершено.\n",
      "Лучшая модель сохранена в: ./trained_models_colab/model_20class_SessionAllLayers/model_20class_SessionAllLayers.keras\n",
      "Загрузка лучшей сохраненной модели для оценки/экспорта...\n",
      "Лучшая модель загружена.\n",
      "\n",
      "Начало оценки модели на тестовых данных...\n",
      "\u001b[1m446/446\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--- Результаты Оценки ---\n",
      "Общая точность (Accuracy): 0.9443\n",
      "Precision (Macro Avg):    0.9491\n",
      "Recall (Macro Avg):       0.9458\n",
      "F1-Score (Macro Avg):     0.9461\n",
      "\n",
      "Метрики по классам:\n",
      "Класс  Имя                Precision  Recall     F1-Score   Support \n",
      "----------------------------------------------------------------------\n",
      "0      BitTorrent         0.8273     0.9109     0.8671     752     \n",
      "1      Facetime           1.0000     0.9767     0.9882     600     \n",
      "2      FTP                1.0000     1.0000     1.0000     689     \n",
      "3      Gmail              0.8989     0.7416     0.8127     863     \n",
      "4      MySQL              1.0000     1.0000     1.0000     724     \n",
      "5      Outlook            0.8249     0.9149     0.8676     752     \n",
      "6      Skype              0.9814     1.0000     0.9906     632     \n",
      "7      SMB                0.9916     0.9305     0.9601     633     \n",
      "8      Weibo              0.9361     0.9880     0.9613     667     \n",
      "9      WorldOfWarcraft    1.0000     0.9949     0.9975     788     \n",
      "10     Cridex             1.0000     0.9903     0.9951     822     \n",
      "11     Geodo              0.9794     0.9910     0.9852     670     \n",
      "12     Htbot              0.9870     0.9560     0.9713     637     \n",
      "13     Miuref             0.9622     0.9922     0.9770     513     \n",
      "14     Neris              0.7735     0.9082     0.8355     850     \n",
      "15     Nsis-ay            0.9895     0.9325     0.9601     607     \n",
      "16     Shifu              0.9948     0.9927     0.9938     963     \n",
      "17     Tinba              0.9930     0.9976     0.9953     850     \n",
      "18     Virut              0.8425     0.7055     0.7679     652     \n",
      "19     Zeus               1.0000     0.9934     0.9967     603     \n",
      "\n",
      "Результаты оценки записаны в out_tf2.txt\n",
      "-------------------------\n",
      "Конвертация модели в ONNX по пути: ./trained_models_colab/model_20class_SessionAllLayers/model_20class_SessionAllLayers.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1744123766.172426  261786 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\n",
      "I0000 00:00:1744123766.172588  261786 single_machine.cc:374] Starting new session\n",
      "I0000 00:00:1744123766.172973  261786 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13553 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Ti SUPER, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "I0000 00:00:1744123766.252742  261786 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13553 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Ti SUPER, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "I0000 00:00:1744123766.262751  261786 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\n",
      "I0000 00:00:1744123766.262884  261786 single_machine.cc:374] Starting new session\n",
      "I0000 00:00:1744123766.263242  261786 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13553 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Ti SUPER, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "rewriter <function rewrite_constant_fold at 0x7f6c265db400>: exception `np.cast` was removed in the NumPy 2.0 release. Use `np.asarray(arr, dtype=dtype)` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX-модель успешно сохранена.\n",
      "Пропуск проверки ONNX: экспорт не удался или был пропущен.\n",
      "\n",
      "Скрипт завершен.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # --- Параметры ---\n",
    "    # Задайте путь к вашим данным на Google Drive\n",
    "    # Убедитесь, что ваш Google Drive смонтирован в Colab\n",
    "    # Пример: from google.colab import drive; drive.mount('/content/drive')\n",
    "    DATA_DIR = \"./dataset/20class/SessionAllLayers\"\n",
    "\n",
    "    CLASS_NUM = 20              # Количество классов\n",
    "    TRAIN_STEPS = 2000         # Общее количество шагов обучения (TRAIN_ROUND)\n",
    "    BATCH_SIZE = 50             # Размер батча\n",
    "    MODEL_SAVE_DIR = './trained_models_colab' # Директория для сохранения моделей в Colab\n",
    "    LOAD_WEIGHTS_PATH = False #\"./trained_models_colab/model_20class_SessionAllLayers_logits/model_20class_SessionAllLayers_logits.keras\"    # Укажите путь к .h5 файлу, если хотите загрузить веса, например: '/content/drive/MyDrive/мои_веса.h5'\n",
    "    SKIP_TRAIN = False          # Установите True, если хотите пропустить обучение (требует LOAD_WEIGHTS_PATH)\n",
    "    EXPORT_ONNX = True          # Экспортировать ли модель в ONNX\n",
    "    TEST_ONNX = True            # Проверять ли ONNX модель (требует EXPORT_ONNX)\n",
    "    NO_SOFTMAX = False           # Создать модель БЕЗ финального Softmax (Рекомендуется для NMDL)\n",
    "    # ------------------\n",
    "\n",
    "    # Проверка аргументов (логика из парсера)\n",
    "    if SKIP_TRAIN and not LOAD_WEIGHTS_PATH:\n",
    "        print(\"Ошибка: SKIP_TRAIN установлен в True, но LOAD_WEIGHTS_PATH не указан.\")\n",
    "        sys.exit(1)\n",
    "    if TEST_ONNX and not EXPORT_ONNX:\n",
    "        print(\"Предупреждение: TEST_ONNX установлен в True, но EXPORT_ONNX в False. Экспорт будет выполнен.\")\n",
    "        EXPORT_ONNX = True\n",
    "\n",
    "    print(\"--- Параметры Запуска ---\")\n",
    "    print(f\"DATA_DIR:          {DATA_DIR}\")\n",
    "    print(f\"CLASS_NUM:         {CLASS_NUM}\")\n",
    "    print(f\"TRAIN_STEPS:       {TRAIN_STEPS}\")\n",
    "    print(f\"BATCH_SIZE:        {BATCH_SIZE}\")\n",
    "    print(f\"MODEL_SAVE_DIR:    {MODEL_SAVE_DIR}\")\n",
    "    print(f\"LOAD_WEIGHTS_PATH: {LOAD_WEIGHTS_PATH}\")\n",
    "    print(f\"SKIP_TRAIN:        {SKIP_TRAIN}\")\n",
    "    print(f\"EXPORT_ONNX:       {EXPORT_ONNX}\")\n",
    "    print(f\"TEST_ONNX:         {TEST_ONNX}\")\n",
    "    print(f\"NO_SOFTMAX:        {NO_SOFTMAX}\")\n",
    "    print(\"-------------------------\")\n",
    "\n",
    "    # Загрузка данных\n",
    "    try:\n",
    "        data_loader = MNISTDataLoader(DATA_DIR, one_hot=True, class_num=CLASS_NUM)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Ошибка: Директория данных или файлы не найдены! {e}\")\n",
    "        sys.exit(1)\n",
    "    except Exception as e:\n",
    "         print(f\"Непредвиденная ошибка при загрузке данных: {e}\")\n",
    "         sys.exit(1)\n",
    "\n",
    "    # Определяем пути для сохранения\n",
    "    folder_name = os.path.basename(DATA_DIR.rstrip('/')) or f\"data_{CLASS_NUM}class\" # Убираем слэш в конце если есть\n",
    "    model_base_name = f\"model_{CLASS_NUM}class_{folder_name}\"\n",
    "    if NO_SOFTMAX:\n",
    "        model_base_name += \"_logits\"\n",
    "    # Убедимся, что основная директория для сохранения существует\n",
    "    os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "    model_save_sub_dir = os.path.join(MODEL_SAVE_DIR, model_base_name) # Поддиректория для конкретной модели\n",
    "    model_save_path = os.path.join(model_save_sub_dir, model_base_name + \".keras\")\n",
    "    onnx_save_path = os.path.join(model_save_sub_dir, model_base_name + \".onnx\")\n",
    "\n",
    "    # Создание модели\n",
    "    model = create_original_model(CLASS_NUM, include_softmax=(not NO_SOFTMAX))\n",
    "\n",
    "    # Обучение или загрузка весов\n",
    "    if not SKIP_TRAIN:\n",
    "        if LOAD_WEIGHTS_PATH:\n",
    "            if os.path.exists(LOAD_WEIGHTS_PATH):\n",
    "                try:\n",
    "                    print(f\"Загрузка весов из: {LOAD_WEIGHTS_PATH}\")\n",
    "                    # Загружаем только веса, предполагая совпадение архитектуры\n",
    "                    model.load_weights(LOAD_WEIGHTS_PATH)\n",
    "                    print(\"Веса загружены успешно.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Ошибка загрузки весов из {LOAD_WEIGHTS_PATH}: {e}\")\n",
    "                    print(\"Начинаем обучение с нуля.\")\n",
    "                    # Переходим к обучению, т.к. загрузка не удалась\n",
    "                    train_model(model, data_loader, TRAIN_STEPS, BATCH_SIZE, model_save_path)\n",
    "            else:\n",
    "                print(f\"Предупреждение: Файл весов {LOAD_WEIGHTS_PATH} не найден. Начинаем обучение с нуля.\")\n",
    "                train_model(model, data_loader, TRAIN_STEPS, BATCH_SIZE, model_save_path)\n",
    "        else:\n",
    "            # Обучаем с нуля\n",
    "            train_model(model, data_loader, TRAIN_STEPS, BATCH_SIZE, model_save_path)\n",
    "            print(f\"Лучшая модель сохранена в: {model_save_path}\")\n",
    "            # Загружаем лучшую модель после обучения для последующих шагов\n",
    "            print(\"Загрузка лучшей сохраненной модели для оценки/экспорта...\")\n",
    "            try:\n",
    "                # Перезагружаем модель, которая была сохранена колбэком\n",
    "                # Компиляция сохранится, если save_weights_only=False (по умолчанию)\n",
    "                model = tf.keras.models.load_model(model_save_path)\n",
    "                print(\"Лучшая модель загружена.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Ошибка загрузки лучшей модели из {model_save_path}: {e}\")\n",
    "                print(\"Оценка и экспорт будут выполнены с моделью в текущем состоянии.\")\n",
    "\n",
    "\n",
    "    elif LOAD_WEIGHTS_PATH: # Если пропустили трейн, но указали веса\n",
    "         if os.path.exists(LOAD_WEIGHTS_PATH):\n",
    "             try:\n",
    "                 print(f\"Загрузка весов из: {LOAD_WEIGHTS_PATH}\")\n",
    "                 # Загружаем только веса в созданную архитектуру\n",
    "                 model.load_weights(LOAD_WEIGHTS_PATH)\n",
    "                 print(\"Веса загружены успешно.\")\n",
    "             except Exception as e:\n",
    "                 print(f\"Ошибка загрузки весов из {LOAD_WEIGHTS_PATH}: {e}\")\n",
    "                 sys.exit(1)\n",
    "         else:\n",
    "             print(f\"Ошибка: Файл весов {LOAD_WEIGHTS_PATH} не найден, обучение пропущено.\")\n",
    "             sys.exit(1)\n",
    "    else:\n",
    "        print(\"Ошибка: Обучение пропущено (SKIP_TRAIN=True), но не указан путь для загрузки весов (LOAD_WEIGHTS_PATH).\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Оценка модели (уже обученной или с загруженными весами)\n",
    "    if model: # Убедимся, что модель существует\n",
    "        evaluate_model(model, data_loader, CLASS_NUM, DATA_DIR)\n",
    "    else:\n",
    "        print(\"Ошибка: Модель не была обучена или загружена. Пропуск оценки.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "    # Экспорт в ONNX\n",
    "    onnx_exported = False\n",
    "    if EXPORT_ONNX and model:\n",
    "       os.makedirs(model_save_sub_dir, exist_ok=True) # Убедимся что директория есть\n",
    "       onnx_exported = convert_to_onnx(model, onnx_save_path) # Передаем class_num\n",
    "\n",
    "    # Проверка ONNX\n",
    "    if TEST_ONNX and onnx_exported:\n",
    "        test_onnx_model(onnx_save_path, data_loader)\n",
    "    elif TEST_ONNX and not onnx_exported:\n",
    "        print(\"Пропуск проверки ONNX: экспорт не удался или был пропущен.\")\n",
    "\n",
    "    print(\"\\nСкрипт завершен.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_with_onnx_model(onnx_path: str, input_data: np.ndarray, input_name: str = \"input\") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Выполняет инференс ONNX-модели через onnxruntime.\n",
    "\n",
    "    Args:\n",
    "        onnx_path (str): Путь к .onnx файлу.\n",
    "        input_data (np.ndarray): Входные данные формы (batch_size, 784).\n",
    "        input_name (str): Имя входного тензора (по умолчанию \"input\").\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Предсказания модели.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Сессия инференса\n",
    "        session = ort.InferenceSession(onnx_path)\n",
    "        input_name = session.get_inputs()[0].name\n",
    "        output_name = session.get_outputs()[0].name\n",
    "\n",
    "        # Инференс\n",
    "        output = session.run([output_name], {input_name: input_data.astype(np.float32)})\n",
    "        return output[0]\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Ошибка при ONNX-инференсе: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Ошибка при ONNX-инференсе: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Got invalid dimensions for input: input for the following indices\n",
      " index: 0 Got: 2 Expected: 1\n",
      " Please fix either the inputs/outputs or the model.\n",
      "🧠 ONNX Predictions:\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "predictions = infer_with_onnx_model(onnx_save_path, np.random.rand(2, 784).astype(np.float32))\n",
    "print(\"🧠 ONNX Predictions:\")\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
